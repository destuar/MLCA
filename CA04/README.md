# Ensemble Methods Comparison Notebook

This Jupyter notebook provides an in-depth comparison and analysis of various ensemble machine learning models. The aim is to evaluate the performance of each model on a given dataset and to determine the optimal parameters that yield the best performance. The notebook covers the following ensemble methods:

## Contents

1. **Random Forest**
   - Performance Metrics
   - Optimal value of the estimator within the given range

2. **Ada Boost**
   - Performance Metrics
   - Optimal Value of the Estimator Within the Given Range

3. **Gradient Boost**
   - Performance Metrics
   - Optimal Value of the Estimator Within the Given Range

4. **XGBoost (XGB)**
   - Performance Metrics
   - Optimal Value of the Estimator Within the Given Range

## Objective

The main objective of this notebook is to provide a comparative analysis of different ensemble methods to identify which model performs best for the specific dataset provided. The comparison focuses on various performance metrics and the optimal values of estimators within given ranges.

## Data Usage

The models are evaluated using a dataset that is relevant to the problem statement addressed in this notebook. The link for the dataset is: "https://github.com/ArinB/MSBA-CA-03-Decision-Trees/blob/master/census_data.csv?raw=true"


## How to Use

To replicate the analysis:
- Ensure you have Jupyter Notebook or JupyterLab installed.
- Clone this repository or download the `CA04_v2.ipynb` file.
- Open the notebook in Jupyter and run the cells sequentially.

## Requirements

- Python 3.x
- Libraries: numpy, pandas, scikit-learn, xgboost (ensure these are installed in your environment)

## Conclusion

The notebook concludes with a discussion on the performance of each model, highlighting the strengths and weaknesses of each approach. This analysis should serve as a guide to selecting the most appropriate ensemble method for similar datasets.
